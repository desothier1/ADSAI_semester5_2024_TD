{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter_02_Assigment_Key.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Assignments Week 2 - Key"
      ],
      "metadata": {
        "id": "gc7jaTy-9XzV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrLDOd46a5Uy"
      },
      "source": [
        "## Assignment 1: Extract n-grams\n",
        "Write a function \"ngrams\", which retrieves the list of n-grams for a given input sentence, for any value of n."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6PrJSqYa5Uz",
        "outputId": "63cd2d8d-d3e3-4c61-fea7-933665a042fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "Bigrams:\n",
            "[['<s>', 'this'], ['this', 'is'], ['is', 'a'], ['a', 'test'], ['test', 'sentence'], ['sentence', '.'], ['.', '</s>']]\n",
            "Trigrams:\n",
            "[['<s>', '<s>', 'this'], ['<s>', 'this', 'is'], ['this', 'is', 'a'], ['is', 'a', 'test'], ['a', 'test', 'sentence'], ['test', 'sentence', '.'], ['sentence', '.', '</s>'], ['.', '</s>', '</s>']]\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "sentence = \"this is a test sentence.\" \n",
        "\n",
        "def ngrams(sentence, n):\n",
        "    ngrams_list = []\n",
        "    \n",
        "    tokens = nltk.word_tokenize(sentence)\n",
        "    tokens = [\"<s>\"]*(n-1) + tokens + [\"</s>\"]\n",
        "    \n",
        "    for i in range(0, len(tokens)-n+1):\n",
        "        ngram = tokens[i:i+n]\n",
        "        ngrams_list.append(ngram)\n",
        "    return ngrams_list\n",
        "\n",
        "print(\"Bigrams:\")\n",
        "print(ngrams(sentence, 2))\n",
        "print(\"Trigrams:\")\n",
        "print(ngrams(sentence, 3))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBHHuc2fa5U6"
      },
      "source": [
        "## Assignment 2 - Predict the next word \n",
        "Write a function \"predict_next_word\" to get phrase (of at least two words long) from the user as input and predict the most probably next word by analyzing 3-grams in the 3-gram dictionary we created, in which the word is the last word (i.e. we look at the last two words or the 2-gram to predict the 3rd word).\n",
        "\n",
        "If there are no valid options (no probable words can be found), print an error message.\n",
        "Print the suggested word and the probability of the trigram ending with it.\n",
        "\n",
        "Example input: __this is__ <br>\n",
        "Output:<br>\n",
        "__a (prob. 0.1428)__<br>\n",
        "<br>\n",
        "Example input: __i like__ <br>\n",
        "Output:<br>\n",
        "__him (prob. 0.2727)__<br>\n",
        "<br>\n",
        "Example input: __i like to eat__ <br>\n",
        "Output:<br>\n",
        "__go (prob. 0.1086)__ (this prob is calculated by only looking at the two previous words \"to eat\".<br>\n",
        "<br>\n",
        "Example input: __when sometimes tomorrow__ <br>\n",
        "Output:<br>\n",
        "__No words could be predicted!__<br>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZpk72Bea5U6",
        "outputId": "7cf6aa9d-63d8-4d51-e4f0-0fe14a3a67ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please provide an unfinished sentence of at least two words long: this is\n",
            "a (prob. 0.14285714285714285)\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from operator import itemgetter\n",
        "\n",
        "with open('./3gram_dict.json', 'r') as fp:\n",
        "    emma_word_prob_3 = json.load(fp)\n",
        "\n",
        "def predict_next_word():\n",
        "    phrase = input(\"Please provide an unfinished sentence of at least two words long: \")\n",
        "    # First we get the tokens\n",
        "    tokens = nltk.word_tokenize(phrase)\n",
        "\n",
        "    # and the get the last two words\n",
        "    bigram = tokens[-2:]\n",
        "    #print(bigram)\n",
        "\n",
        "    # We can collect all 3-grams in our dictionary starting with these two words to a list\n",
        "    candidates = []\n",
        "    for key, value in emma_word_prob_3.items():\n",
        "        # We need to merge the two-gram to a single string so that we can match them as keys in the dictionary\n",
        "        if key.startswith(\" \".join(bigram)):\n",
        "            # If we find a match, add the key to the candidates list\n",
        "            candidates.append([key, value])\n",
        "\n",
        "    # If there are no candidates, print a message\n",
        "    if len(candidates) == 0:\n",
        "        print(\"No words could be predicted!\")\n",
        "\n",
        " \n",
        "    candidates_sorted  = sorted(candidates, key=itemgetter(1), reverse=True) # sort the list of lists by probability\n",
        "    \n",
        "    word = candidates_sorted[0][0].split()[-1]\n",
        "    prob = candidates_sorted[0][1]\n",
        "    print(word, \"(prob. \"+ str(prob) + \")\")\n",
        "    \n",
        "\n",
        "\n",
        "predict_next_word()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9kBKcXxfa5U6"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oq_pQOjLa5U7"
      },
      "source": [
        "## Assignment 3 - Predict the next word continued\n",
        "Let's modify the code above to print 5 most probably words as the next word.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZP-SQHbGa5U7",
        "outputId": "bc4e7954-7b8d-4926-ea28-4ac30061a3f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please provide an unfinished sentence of at least two words long: this is\n",
            "[['this is a', 0.14285714285714285], ['this is quite', 0.08928571428571429], ['this is the', 0.08928571428571429], ['this is an', 0.05357142857142857], ['this is not', 0.05357142857142857], ['this is what', 0.05357142857142857], ['this is all', 0.03571428571428571], ['this is brilliant', 0.03571428571428571], ['this is just', 0.03571428571428571], ['this is most', 0.03571428571428571], ['this is very', 0.03571428571428571], ['this is admirable', 0.017857142857142856], ['this is by', 0.017857142857142856], ['this is charming', 0.017857142857142856], ['this is coming', 0.017857142857142856], ['this is delightful', 0.017857142857142856], ['this is feeling', 0.017857142857142856], ['this is from', 0.017857142857142856], ['this is great', 0.017857142857142856], ['this is impossible', 0.017857142857142856], ['this is in', 0.017857142857142856], ['this is like', 0.017857142857142856], ['this is living', 0.017857142857142856], ['this is meeting', 0.017857142857142856], ['this is precisely', 0.017857142857142856], ['this is saying', 0.017857142857142856], ['this is such', 0.017857142857142856], ['this is three', 0.017857142857142856], ['this is too', 0.017857142857142856], ['this is your', 0.017857142857142856]]\n",
            "a (prob. 0.14285714285714285)\n",
            "quite (prob. 0.08928571428571429)\n",
            "the (prob. 0.08928571428571429)\n",
            "an (prob. 0.05357142857142857)\n",
            "not (prob. 0.05357142857142857)\n"
          ]
        }
      ],
      "source": [
        "from operator import itemgetter\n",
        "\n",
        "def predict_next_word():\n",
        "    phrase = input(\"Please provide an unfinished sentence of at least two words long: \")\n",
        "    # First we get the tokens\n",
        "    tokens = nltk.word_tokenize(phrase)\n",
        "\n",
        "    # and the get the last two words\n",
        "    bigram = tokens[-2:]\n",
        "    #print(bigram)\n",
        "\n",
        "    # We can collect all 3-grams in our dictionary starting with these two words to a list\n",
        "    candidates = []\n",
        "    for key, value in emma_word_prob_3.items():\n",
        "        # We need to merge the two-gram to a single string so that we can match them as keys in the dictionary\n",
        "        if key.startswith(\" \".join(bigram)):\n",
        "            # If we find a match, add the key to the candidates list\n",
        "            candidates.append([key, value])\n",
        "\n",
        "    # If there are no candidates, print a message\n",
        "    if len(candidates) == 0:\n",
        "        print(\"No words could be predicted!\")\n",
        "\n",
        " \n",
        "    candidates_sorted  = sorted(candidates, key=itemgetter(1), reverse=True) # sort the list of lists by probability\n",
        "    print(candidates_sorted)\n",
        "    for i in range(5):\n",
        "        word = candidates_sorted[i][0].split()[-1]\n",
        "        prob = candidates_sorted[i][1]\n",
        "        print(word, \"(prob. \"+ str(prob) + \")\")\n",
        "    \n",
        "\n",
        "\n",
        "predict_next_word()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nvJ6zXwa5U7"
      },
      "source": [
        "## Assignment 4: Text Generation \n",
        "Let's generate some \"good\" sentences of 10 words long by using random ngrams from a given ngram dictionary.\n",
        "\n",
        "- Start with a random n-gram from the dictionary\n",
        "- Add words to sentence by finding probable words that follow the previous n-1 grams, until 10 words are generated.\n",
        "- if it is not possible to find a probable next word at some point, print the sentence generated so far and print a message \"No other words could be added.\".\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPfX3FLKa5U7",
        "outputId": "038a38d2-529a-43f6-92ae-b3f451585de4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "her : you will stay , yes -- it _is_ possible --\"\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import random\n",
        "import json\n",
        "import sys\n",
        "\n",
        "with open('./3gram_dict.json', 'r') as fp:\n",
        "    emma_word_prob_3 = json.load(fp)\n",
        "\n",
        "\n",
        "def make_random_sentence(prob_dict, n):\n",
        "    sentence = []\n",
        "    for i in range(10):\n",
        "        #print(i)\n",
        "        if i == 1:\n",
        "            #print(\"First n-gram:\")\n",
        "            ngram = random.choice(list(prob_dict))\n",
        "            tokens = ngram.split()\n",
        "            #print(ngram)\n",
        "            for token in tokens:\n",
        "                sentence.append(token)\n",
        "        else:\n",
        "            \n",
        "            #print(sentence)\n",
        "            start = sentence[-n+1:] # get the last n-1 tokens to find a new ngram that starts with it\n",
        "            #print(start)\n",
        "            candidates = []\n",
        "            for key in prob_dict:\n",
        "                if key.startswith(\" \".join(start)):\n",
        "                    candidates.append(key)\n",
        "            try:\n",
        "                ngram = random.choice(candidates)\n",
        "                #print(ngram)\n",
        "                word = ngram.split()[-1]\n",
        "                sentence.append(word)\n",
        "                #print(sentence)\n",
        "            except:\n",
        "                text = \" \".join(sentence)\n",
        "                print(text)\n",
        "                print(\"No other words could be added.\")\n",
        "                sys.exit()\n",
        "            \n",
        "       \n",
        "\n",
        "    text = \" \".join(sentence)\n",
        "    print(text)\n",
        "\n",
        "make_random_sentence(emma_word_prob_3, 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2F0ria4Ra5U7"
      },
      "source": [
        "## Assignment 5 - Text generation continued\n",
        "Adapt the code above so that:\n",
        "- it starts with a random n-gram that begins with \"(s)\"\n",
        "- it restarts generating a sentence if no valid ngrams are found during generation of a sentence of 10 words.\n",
        "    \n",
        "Tip: Which one is better: a for loop or a while loop. Why?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VCKGQMk1a5U8",
        "outputId": "65fbc6e0-00bf-4a04-a2af-4556ef683406",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s> <s> randalls is doubly due , and throw up a she\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "def make_random_sentence(prob_dict, n):\n",
        "\n",
        "    sentence = []\n",
        "    i = 0\n",
        "    while i < 10:\n",
        "        i+=1\n",
        "        #print(i)\n",
        "        if i == 1:\n",
        "            start_candidates = []\n",
        "            for key, value in prob_dict.items():\n",
        "                if key.startswith(\"<s> <s>\"):\n",
        "                    start_candidates.append(key)\n",
        "            ngram = random.choice(start_candidates)\n",
        "            tokens = ngram.split()\n",
        "            for token in tokens:\n",
        "                sentence.append(token)\n",
        "        else:\n",
        "            try:\n",
        "                #print(sentence)\n",
        "                start = sentence[-n+1:] # get the last n-1 tokens to find a new ngram that starts with it\n",
        "                #print(start)\n",
        "                candidates = []\n",
        "                for key in prob_dict:\n",
        "                    if key.startswith(\" \".join(start)):\n",
        "                        candidates.append(key)\n",
        "                ngram = random.choice(candidates)\n",
        "                #print(ngram)\n",
        "                word = ngram.split()[-1]\n",
        "                sentence.append(word)\n",
        "            except:\n",
        "                print(\"resetting ... \")\n",
        "                print(\" \".join(sentence))\n",
        "                i = 0\n",
        "                sentence = []\n",
        "       \n",
        "\n",
        "    text = \" \".join(sentence)\n",
        "    print(text)\n",
        "\n",
        "make_random_sentence(emma_word_prob_3, 3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "-spN1Z88DAjj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}