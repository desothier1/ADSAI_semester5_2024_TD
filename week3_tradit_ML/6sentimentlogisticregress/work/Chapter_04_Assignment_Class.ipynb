{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter_04_Assignment-Class.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 04 Logistic Regression: Assignment"
      ],
      "metadata": {
        "id": "YOssXJqxeCuj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Following files need to be uploaded to the Colab environment first:\n",
        "- logreg_nouns.pkl (Logistic Regression Model)\n",
        "- train.csv (training data)\n",
        "- test.csv (test data)"
      ],
      "metadata": {
        "id": "wzsH6uZQgj_Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We first load the logistic regression model we have trained during the class, which uses nouns as features."
      ],
      "metadata": {
        "id": "seBCBwetfbng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "# Load from file\n",
        "with open(\"logreg_nouns.pkl\", 'rb') as file:\n",
        "    lr_nouns = pickle.load(file)\n",
        "    \n",
        "# Let's see what are the possible labels to predict (and in which order they are stored)\n",
        "print(lr_nouns.classes_)\n",
        "\n",
        "# We can get additional information about all the parameters used with LogReg model\n",
        "print(lr_nouns.get_params())"
      ],
      "metadata": {
        "id": "HpmKGAd1fkoO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also, we create the list of nouns (unique, lemmatised) by processing the training data."
      ],
      "metadata": {
        "id": "fO9iLtHkgNbN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas\n",
        "\n",
        "# Let's load the training data from a csv file\n",
        "train_set = pandas.read_csv('./train.csv', sep='\\t', encoding='utf-8')\n",
        "test_set = pandas.read_csv('./test.csv', sep='\\t', encoding='utf-8')\n",
        "test_set"
      ],
      "metadata": {
        "id": "gWOWH8lcgaOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's extract only these two columns from the data \n",
        "train_reviews = train_set['Review'].to_list()\n",
        "train_colors = train_set['Color'].to_list()\n",
        "\n",
        "test_reviews = test_set['Review'].to_list()\n",
        "test_colors = test_set['Color'].to_list()\n"
      ],
      "metadata": {
        "id": "MXRfLqLWg04T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Process a text\n",
        "train_doc_reviews = nlp.pipe(train_reviews)\n",
        "\n",
        "list_noun_lemma = []\n",
        "\n",
        "for review in train_doc_reviews:\n",
        "    for token in review:\n",
        "        if token.pos_ == \"NOUN\":\n",
        "            # In this case, we will add the lemma of the noun to our list and not the full word\n",
        "            list_noun_lemma.append(token.lemma_)\n",
        "            \n",
        "\n",
        "# We are only interested in the list of unique nouns\n",
        "list_noun_lemma_unique = list(set(list_noun_lemma))\n",
        "\n",
        "# Let's print and see how many unique nouns we have\n",
        "print(len(list_noun_lemma_unique))\n",
        "print(list_noun_lemma_unique)"
      ],
      "metadata": {
        "id": "URsY8kj_gMtL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wD1ctxX7HTp5"
      },
      "source": [
        "## Assignment 1. Feature weights ##\n",
        "Can you print the 5 terms (nouns in this case) that have the largest weight (coefficients) for predicting the label \"Red\"? (Use the logistic regression model that was trained with NOUNS)\n",
        "- Print largest absolute weights (weights can be positive or negative)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tgnWtu3tHTp5"
      },
      "outputs": [],
      "source": [
        "# Insert your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjtQcl5GHTp5"
      },
      "source": [
        "## Assignment 2. Feature weights ##\n",
        "Can you print the 10 terms (nouns) that have the largest weight (coefficients) for each label separately: 'Red' 'Rose' 'White' 'unk'. (Use the logistic regression model that was trained with NOUNS)\n",
        "- Print largest absolute weights (weights can be positive or negative)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_M4bHr3aHTp5"
      },
      "outputs": [],
      "source": [
        "# Insert your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YHDfsldHTp5"
      },
      "source": [
        "## Assignment 3. Adjectives ##\n",
        "Train a new model that only uses the adjectives in the test set as features (lowercased, unique, lemmatized).\n",
        "Remove the adjectives \"red\", \"white\" and \"rose\" from the unique list of adjectives (we don't want to predict the labels from the same words).\n",
        "\n",
        "Save your model! We will evaluate them next week!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVddsdyqHTp5"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Process a text\n",
        "test_doc_reviews = nlp.pipe(test_reviews)\n",
        "\n",
        "list_adj = []\n",
        "\n",
        "# Fill the rest of the code\n",
        "# Feel free to use additional cells not to repeat steps that take long time (such as generating features)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Extract a list of unique adjective lemmas from the training set (which will be used as features for this model)"
      ],
      "metadata": {
        "id": "0MHRPMVFd_l6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Remove the adjectives 'red', 'white' and 'rose' from the extracted list\n"
      ],
      "metadata": {
        "id": "qde8Jmn7kI_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Create zero-vectors with the right shape for the train and test feature sets"
      ],
      "metadata": {
        "id": "sCLM0ZuukPTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Assign correct values in the feature vectors for the training set"
      ],
      "metadata": {
        "id": "wFUjquKGkWWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Assign correct values in the feature vectors for the test set"
      ],
      "metadata": {
        "id": "LNugn9LCke-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Train a logistic regression model using the features and the (color) labels for the training set"
      ],
      "metadata": {
        "id": "ZfjH903ZkhdF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Define a function which takes an index value as input and predicts color for the review in the test set with this index"
      ],
      "metadata": {
        "id": "MPnmXMaQko2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8: Save the model as \"logreg_adj_assignment.pkl\""
      ],
      "metadata": {
        "id": "gUqpx3l6k1xY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}