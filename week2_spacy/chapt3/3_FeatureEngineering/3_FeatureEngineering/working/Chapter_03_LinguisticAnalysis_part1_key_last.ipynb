{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Linguistic Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have been analysing natural language by looking at surface forms of words (i.e. forms of words as they appear in the text).\n",
    "To be able to write good NLP applications, we need linguistic data.\n",
    "There are many tools to perform linguistic analysis of texts in different levels (NLTK, CoreNLP, Gensim, SpaCy etc.).\n",
    "We will focus on SpaCy to do this!\n",
    "\n",
    "With SpaCy, we can build a text processing pipeline, which will involve:\n",
    "- Sentence segmentation\n",
    "- Tokenization\n",
    "- Part-of-speech tagging\n",
    "- Dependency parsing\n",
    "- Named entity recognition\n",
    "\n",
    "SpaCy can be installed with:<br>\n",
    "__pip install spacy__\n",
    "\n",
    "Also, we will use an English model. On command line: <br>\n",
    "__python3 -m spacy download en_core_web_sm__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2.1\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "print(spacy.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before getting started: Python Classes/Objects\n",
    "\n",
    "Python is an object-oriented language (object-oriented programming, OOP).\n",
    "Objects are an encapsulation of __variables__ and __functions__ into a single entity. Objects are instances of classes. In other words, a class is a template to create your objects.\n",
    "The concept of OOP in Python focuses on creating reusable code.\n",
    "\n",
    "An object has two characteristics:\n",
    "\n",
    "- attributes\n",
    "- behavior (methods)\n",
    "\n",
    "To create a class, use the keyword class.\n",
    "Create a class \"Person\", with a property called \"name\":\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Person:\n",
    "    name = \"John\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the class named Person to create objects (instances of the class).\n",
    "Let's Create an object named p1, and print the value of \"name\":\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John\n"
     ]
    }
   ],
   "source": [
    "#create instance or object of class 'Person'\n",
    "p1 = Person()\n",
    "print(p1.name)\n",
    "\n",
    "#in fact similar to creating a dictionary:\n",
    "#d1 = dict()\n",
    "#you create instance of the class dict\n",
    "#below are some methods that can be applied to objects of that class:\n",
    "\n",
    "#however, with this code we will always have instance 'john'\n",
    "#see below, with init..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are some objects we know in Python: lists, dictionaries, tuples, strings etc. \n",
    "    list.append()\n",
    "    list.remove()\n",
    "    list.sort()\n",
    "    \n",
    "    dict.keys()\n",
    "    dict.values()\n",
    "    dict.items()\n",
    "    \n",
    "    string.lower()\n",
    "    string.strip()\n",
    "    string.split()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The examples above are classes and objects  are not really useful in real life applications. If we create a \"Person\" object, it's name is always \"John\". Ideally, we would like to create objects by assigning values to their attributes (properties).\n",
    "\n",
    "All classes have a special method called __\\_\\_init\\_\\_()__, which is always executed when the class is being initiated.\n",
    "Let's use the __\\_\\_init\\_\\_()__ function to create a \"Person\" object by assigning values for its \"name\" and \"age\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "class Person:\n",
    "    #class is a kind of template of which objects can be derived\n",
    "    def __init__(self, name, age):\n",
    "        self.name = name\n",
    "        self.age = age\n",
    "        #self refers to object\n",
    "        #age and name are attributes\n",
    "        #init is a method that initiates a class\n",
    "\n",
    "# Create a new person object\n",
    "\n",
    "p1 = Person(\"John\", 30)\n",
    "#john and 30 are values\n",
    "# Print its attributes\n",
    "\n",
    "print(p1.name)\n",
    "print(p1.age)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John\tSmith\t30\n",
      "\n",
      "George\tWashington\t30\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'p2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_30661/582471512.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mp2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s\\t%s\\t%d\\n\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msurname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'p2' is not defined"
     ]
    }
   ],
   "source": [
    "#TD cheesecake ex.\n",
    "\n",
    "class Person:\n",
    "    #class is a kind of template of which objects can be derived\n",
    "    def __init__(cheesecake, name, surname, age):\n",
    "        cheesecake.name = name\n",
    "        cheesecake.surname = surname        \n",
    "        cheesecake.age = age\n",
    "\n",
    "        #self refers to object\n",
    "        #age and name are attributes\n",
    "        #init is a method that initiates a class\n",
    "\n",
    "# Create a new person object\n",
    "\n",
    "p1 = Person(\"John\", \"Smith\",30)\n",
    "p2 = Person(\"George\", \"Washington\",30)\n",
    "#john and 30 are values\n",
    "# Print its attributes\n",
    "\n",
    "print(\"%s\\t%s\\t%d\\n\"%(p1.name, p1.surname, p1.age))\n",
    "print(\"%s\\t%s\\t%d\\n\"%(p2.name, p2.surname, p2.age))\n",
    "\n",
    "del p2\n",
    "\n",
    "print(\"%s\\t%s\\t%d\\n\"%(p2.name, p2.surname, p2.age))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mammal\n"
     ]
    }
   ],
   "source": [
    "class animal:\n",
    "    def __init__(self,name,subcat):\n",
    "        self.name=name\n",
    "        self.subcat=subcat\n",
    "        \n",
    "\n",
    "examp1=animal(\"tiger\",\"mammal\")\n",
    "\n",
    "print(examp1.subcat)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The __\\_\\_self\\_\\___ parameter is a reference to the current instance of the class, and is used to access variables that belongs to the class.\n",
    "It does not have to be named __\\_\\_self\\_\\___ but it has to be the first parameter of any function in the class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DIY1. Classes and objects\n",
    "\n",
    "1. Modify the \"self\" parameter of the \"Person\" class to \"cheescake\".\n",
    "2. Add the attribute \"surname\" to the \"Person\" class.\n",
    "3. Create a new \"Person\" object (with all the necessary values for its attributes)\n",
    "4. Print the name and surname of the \"Person\" object in a single line\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John Snow\n"
     ]
    }
   ],
   "source": [
    "# Add your code here\n",
    "class Person:\n",
    "    def __init__(cheescake, name, surname, age):\n",
    "        cheescake.name = name\n",
    "        cheescake.age = age\n",
    "        cheescake.surname = surname \n",
    "        \n",
    "# Create a new person object\n",
    "p1 = Person(\"John\", \"Snow\", 30)\n",
    "\n",
    "# Print its attributes\n",
    "\n",
    "print(p1.name + \" \" + p1.surname)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can modify __object__ attributes (properties) directly.\n",
    "Let's modify the age of p1 to 35:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "35\n"
     ]
    }
   ],
   "source": [
    "print(p1.age)\n",
    "\n",
    "# Change the age and print it again\n",
    "\n",
    "p1.age = 35\n",
    "print(p1.age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2883541124.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_30661/2883541124.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    You can also delete objects by using the \"del\" keyword.\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "You can also delete objects by using the \"del\" keyword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'p1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_30661/1314565242.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Delete the person object and print its age again\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mp1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'p1' is not defined"
     ]
    }
   ],
   "source": [
    "print(p1.age)\n",
    "\n",
    "# Delete the person object and print its age again\n",
    "del p1\n",
    "print(p1.age)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add methods to our classes, which can only be used on objects of this class.\n",
    "Let's add a method \"getName\" to our \"Person\" class, which displays the name and surname of a given \"Person\" object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: John, Surname: Snow\n"
     ]
    }
   ],
   "source": [
    "class Person:\n",
    "    def __init__(self, name, surname, age):\n",
    "        self.name = name\n",
    "        self.surname = surname\n",
    "        self.age = age\n",
    "\n",
    "    def getName(self):\n",
    "        #niks na self : inherits\n",
    "        print(\"Name: \" + self.name + \", Surname: \" + self.surname)\n",
    "\n",
    "p1 = Person(\"John\", \"Snow\", 30)\n",
    "\n",
    "p1.getName()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: John, surname: Smith, age: 45\n"
     ]
    }
   ],
   "source": [
    "#TD try\n",
    "\n",
    "\n",
    "class Person:\n",
    "    def __init__(self,name,surname,age):\n",
    "        self.name=name\n",
    "        self.surname=surname\n",
    "        self.age=age\n",
    "    \n",
    "    def getName(self):\n",
    "        print(\"name: \" + self.name + \", surname: \" + self.surname + \", age: \" + self.age)\n",
    "\n",
    "p1 = Person(\"John\",\"Smith\",\"45\")\n",
    "\n",
    "p1.getName()        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: crocodile, category: reptile\n"
     ]
    }
   ],
   "source": [
    "#TD try 2\n",
    "\n",
    "class Animal:\n",
    "    def __init__(self,name,subtype):\n",
    "        self.name=name\n",
    "        self.subtype=subtype\n",
    "        \n",
    "    def getInfo(self):\n",
    "        print(\"name: \" + self.name + \", category: \" + self.subtype)\n",
    "    \n",
    "p1 = Animal(\"crocodile\",\"reptile\")\n",
    "\n",
    "p1.getInfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the __\\_\\_self\\_\\___ parameter __has to be present and be the first variable__ of a method defined in a class.\n",
    "While calling the method on the object itself, the __\\_\\_self\\_\\___ parameter is not used.\n",
    "Let's create:\n",
    "- 1. an attribute for the \"Person\" class called \"pet\"\n",
    "- 2. a method for the \"Person\" class \"changePet\", which modifies the value to the \"pet\" attribute with a given new \"value\" and prints the old and the new pets.\n",
    "\n",
    "In this case, let's make the value of \"pet\" optional within the __\\_\\_init\\_\\()___ method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Ghost\n",
      "Old pet: Ghost\n",
      "New pet: Bobby\n"
     ]
    }
   ],
   "source": [
    "class Person:\n",
    "    def __init__(self, name, surname, age, pet=None):\n",
    "        #pet attribute will be always none by default\n",
    "        self.name = name\n",
    "        self.surname = surname\n",
    "        self.age = age\n",
    "        self.pet = pet\n",
    "        \n",
    "\n",
    "    def getName(self):\n",
    "      print(\"Name: \" + self.name + \", Surname: \" + self.surname)\n",
    "    \n",
    "    #Define a new method changePet\n",
    "    def changePet(self, newpet):\n",
    "        print(\"Old pet: \" +self.pet)\n",
    "        self.pet = newpet\n",
    "        print(\"New pet: \" +self.pet)\n",
    "\n",
    "p1 = Person(\"John\", \"Snow\", 30)\n",
    "print(p1.pet)\n",
    "\n",
    "p1 = Person(\"John\", \"Snow\", 30, \"Ghost\")\n",
    "print(p1.pet)\n",
    "# Create a person object with a \"pet\"\n",
    "\n",
    "# Use changePet method to change the pet\n",
    "p1.changePet(\"Bobby\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age :48\n",
      "age :48\n",
      "age :48\n",
      "name, Thierry surname, Desot age, 48\n"
     ]
    }
   ],
   "source": [
    "#TD inserted\n",
    "import datetime\n",
    "\n",
    "class Person:\n",
    "    def __init__(self, name, surname, birthyear, pet=None):\n",
    "        self.name=name\n",
    "        self.surname=surname\n",
    "        self.birthyear=birthyear\n",
    "        self.pet=pet\n",
    "        \n",
    "    \n",
    "    def getName(self):\n",
    "      print(\"Name: \" + self.name + \", Surname: \" + self.surname)\n",
    "    \n",
    "    def changePet(self, pet):\n",
    "        print(\"Old pet: \"+self.pet)\n",
    "        self.pet = pet\n",
    "        print(\"New pet: \"+self.pet)\n",
    "        \n",
    "    def getAge1(self, year): #year is huidig jaar in te geven        \n",
    "        age = year - self.birthyear\n",
    "        print(\"age :\" + str(age))\n",
    "    \n",
    "    def getAge2(self):\n",
    "        now = datetime.datetime.now()\n",
    "        currentyear = now.year\n",
    "        age = currentyear - self.birthyear\n",
    "        print(\"age :\" + str(age))\n",
    "        return age\n",
    "    \n",
    "    def getSummary(self):\n",
    "        print(\"name, \" + self.name + \" surname, \" + self.surname + \" age, \" + str(self.getAge2()))\n",
    "\n",
    "p1=Person(\"Thierry\",\"Desot\",1974)\n",
    "p1.getAge1(2022)\n",
    "p1.getAge2()\n",
    "p1.getSummary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DIY2. Classes and objects\n",
    "\n",
    "1. Remove the attribute \"age\" and add the attribute \"birthyear\" to the \"Person\" class\n",
    "2. Add a new method \"getAge1\" that calculates the age of a given person using the an input \"year\" and the \"birthdate\" attribute and prints it. \n",
    "3. Add a new method \"getAge2\", which makes the same calculation by using the \"datetime\" library to get the current date and time, without an input variable<br>\n",
    "    - import datetime<br>\n",
    "    - now = datetime.datetime.now()<br>\n",
    "    - now.year # returns current year<br>\n",
    "    - We can calculate the age by using \"birthyear\" and current year.\n",
    "\n",
    "4. Add a new method getSummary, which prints the name, surname and the age in one line (using getAge2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age is 1738\n",
      "Age is 1738\n",
      "Age is 1738\n",
      "John Snow 1738\n"
     ]
    }
   ],
   "source": [
    "#solution\n",
    "import datetime\n",
    "\n",
    "class Person:\n",
    "    # Define the _init_ method\n",
    "    def __init__(self, name, surname, birthyear, pet=None):\n",
    "        self.name = name\n",
    "        self.surname = surname\n",
    "        self.birthyear = birthyear\n",
    "        self.pet = pet\n",
    "        \n",
    "\n",
    "    def getName(self):\n",
    "      print(\"Name: \" + self.name + \", Surname: \" + self.surname)\n",
    "    \n",
    "    def changePet(self, pet):\n",
    "        print(\"Old pet: \"+self.pet)\n",
    "        self.pet = pet\n",
    "        print(\"New pet: \"+self.pet)\n",
    "    \n",
    "    def getAge1(self, year):\n",
    "        age = year - self.birthyear\n",
    "        print(\"Age is \" + str(age))\n",
    "        \n",
    "    def getAge2(self):\n",
    "        now = datetime.datetime.now()\n",
    "        year = now.year\n",
    "        age = year - self.birthyear\n",
    "        print(\"Age is \" + str(age))\n",
    "        return age\n",
    "        \n",
    "    def getSummary(self):\n",
    "        print(self.name + \" \" +  self.surname + \" \" + str(self.getAge2()))\n",
    "        \n",
    "p1 = Person(\"John\", \"Snow\", 283)\n",
    "p1.getAge1(2021)\n",
    "p1.getAge2()\n",
    "p1.getSummary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the advantages of OOP are:\n",
    "\n",
    "__1. Modularity for easier troubleshooting__\n",
    "\n",
    "Something has gone wrong, and you have no idea where to look? Hope you commented your code!\n",
    "\n",
    "When working with object-oriented programming languages, you know exactly where to look:\" “The Person object broke down? The problem must be in the Person class!”.\n",
    "\n",
    "Due to encapsulation, objects are self-contained, and each bit of functionality does its own thing while leaving the other bits alone. Also, this modality allows an IT team to work on multiple objects simultaneously while minimizing the chance that one person might duplicate someone else’s functionality.\n",
    "\n",
    "__2. Reuse of code through inheritance__\n",
    "\n",
    "Suppose that in addition to your Car object, one colleague needs a RaceCar object, and another needs a Limousine object. Everyone builds their objects separately but discover commonalities between them. In fact, each object is really just a different kind of Car. This is where the inheritance technique saves time: Create one generic class (Car), and then define the subclasses (RaceCar and Limousine) that are to inherit the generic class’s traits.\n",
    "\n",
    "Of course, Limousine and RaceCar still have their unique attributes and functions. If the RaceCar object needs a method to “fireAfterBurners” and the Limousine object requires a \"Chauffeur\", each class could implement separate functions just for itself. However, because both classes inherit key aspects from the Car class, for example the “drive” or “fillUpGas” methods, your inheriting classes can simply reuse existing code instead of writing these functions all over again.\n",
    "\n",
    "What if you want to make a change to all Car objects, regardless of type? This is another advantage of the OO approach. Simply make a change to your Car class, and all car objects will simply inherit the new code.\n",
    "\n",
    "__3. Flexibility through polymorphism__\n",
    "\n",
    "Polymorphism is the process of using an operator or function in different ways for different data input. In practical terms, polymorphism means that if class B inherits from class A, it doesn't have to inherit everything about class A; it can do some of the things that class A does differently. Polymorphism is a fancy word that means that the same function can act differently for objects of different types.\n",
    "\n",
    "Let's say we have two animal species: a dog and a cat. Both are animals (class \"Animal\"). The \"Dog\" class and the \"Cat\" class inherit the Animal class. All three classes have a talk() method, which gives different output for each of them. It can for example print \"Animals make different sounds\", when used on an \"Animal\" object. Or it can print \"Meow\" or \"Woof\" when applied to a Cat or a Dog object.\n",
    "\n",
    "__4. Effective problem solving__\n",
    "\n",
    "Object-oriented programming is often the most natural and pragmatic approach, once you get the hang of it. OOP languages allows you to break down your software into bite-sized problems that you then can solve — one object at a time.\n",
    "\n",
    "References:\n",
    "https://www.roberthalf.com/blog/salaries-and-skills/4-advantages-of-object-oriented-programming\n",
    "http://zetcode.com/lang/python/oop/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SpaCy: Getting Started\n",
    "\n",
    "At the center of spaCy is the __object__ containing the processing pipeline. This variable is called \"nlp\".\n",
    "\n",
    "For example, to create an English __nlp object__, you can import the English language class from _spacy.lang.en_ and instantiate it. You can use the nlp object like a function to analyze text. \n",
    "\n",
    "It contains all the different components in the \"NLP pipeline\". When you call \"nlp\" on a text, spaCy first tokenizes the text to produce a __Doc object__. The Doc is then processed in several different steps – this is also referred to as the processing pipeline. The pipeline used by the default models consists of a tagger, a parser and an entity recognizer. Each pipeline component returns the processed Doc, which is then passed on to the next component. One of the nice things about Spacy is that we only need to apply \"nlp\" once to analyze text, the entire background pipeline will return the objects.\n",
    "\n",
    "It also includes language-specific rules used for tokenizing the text into words and punctuation. spaCy supports a variety of languages that are available in spacy.lang.\n",
    "\n",
    "![title](img/pipeline.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world!\n"
     ]
    }
   ],
   "source": [
    "# Import the English language class\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "\n",
    "# Create the nlp object: language specific processing pipeline\n",
    "nlp = English()\n",
    "\n",
    "# Created by processing a string of text with the nlp object\n",
    "doc = nlp(\"Hello world!\")\n",
    "print(doc.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tokenization\n",
    "Token objects represent the tokens in a document – for example, a word or a punctuation character. The tokens are\n",
    "\n",
    "To get a token at a specific position, you can index into the Doc.\n",
    "\n",
    "Token objects also provide various attributes that let you access more information about the tokens. For example, the .text attribute returns the token text.\n",
    "\n",
    "\n",
    "<img src=\"img/doc_token.png\" alt=\"Drawing\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world!\n",
      "Hello\n",
      "world\n",
      "!\n",
      "world\n"
     ]
    }
   ],
   "source": [
    "print(doc.text)\n",
    "# Iterate over tokens in a Doc\n",
    "for token in doc:\n",
    "    print(token.text)\n",
    "    #print(token) # td : also works? \n",
    "# Index into the Doc to get a single Token\n",
    "token = doc[1]\n",
    "\n",
    "# Get the token text via the .text attribute\n",
    "#print(token.text)\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DIY: Tokens ###\n",
    "Fill the question marks in the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trump\n"
     ]
    }
   ],
   "source": [
    "#TD inserted \n",
    "#import the eng language class and create nlp obj\n",
    "#from ??? import ???\n",
    "from spacy.lang.en import English\n",
    "\n",
    "\n",
    "#create nlp object for Eng\n",
    "#nlp = ???\n",
    "nlp = English()\n",
    "\n",
    "#process text\n",
    "#doc = ??? ('\"The winner is a movie from South Korea, what the hell was that all about\" Trump asked.')\n",
    "\n",
    "doc = nlp('\"The winner is a movie from South Korea, what the hell was that all about\" Trump asked.')\n",
    "\n",
    "#select the token \"Trump\" by providing it's index\n",
    "#trump_token = doc[???]\n",
    "trump_token = doc[18]\n",
    "# or trump_token = doc[-3]\n",
    "#print the selected token's text\n",
    "#???\n",
    "print(trump_token.text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trump\n"
     ]
    }
   ],
   "source": [
    "# Import the English language class and create the nlp object\n",
    "from spacy.lang.en import English\n",
    "\n",
    "# Create the nlp object for English\n",
    "nlp = English()\n",
    "\n",
    "# Process the text\n",
    "doc = nlp('\"The winner is a movie from South Korea, what the hell was that all about?\" Trump asked.' )\n",
    "\n",
    "# Select the token \"Trump\" by providing it's index\n",
    "trump_token = doc[-3]\n",
    "\n",
    "# Print the selected token's text\n",
    "print(trump_token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Token does not only have a \"text\" attribute. \n",
    "\n",
    "Here are some of the other token attributes:\n",
    "\n",
    "|Token attribue | Description|\n",
    "|---|:---|\n",
    "|doc|The parent document.|\n",
    "|text|The text content.|\n",
    "|lemma_|Lemma of the token.|\n",
    "|lower_|Lowercase form of the token text. Equivalent to Token.text.lower().|\n",
    "|i|The index of the token within the parent document.|\n",
    "|shape_|Transform of the tokens’s string, to show orthographic features. Alphabetic characters are replaced by x or X, and numeric characters are replaced by d, and sequences of the same character are truncated after length 4. For example,“Xxxx”or“dd”.|\n",
    "|is_alpha|(bool) Does the token consist of alphabetic characters?|\n",
    "|is_punct|(bool) Is the token punctuation?|\n",
    "|like_url|(bool) Does the token resemble a URL?|\n",
    "|like_num|(bool) Does the token represent a number? e.g. “10.9”, “10”, “ten”, etc.|\n",
    "|is_stop|(bool)\tIs the token part of a “stop-word list”?|\n",
    "\n",
    "\n",
    "Note: These attributes are also called lexical attributes: they refer to the entry in the vocabulary and don't depend on the token's context. spaCy encodes all strings to hash values to reduce memory usage and improve efficiency. So to get the readable string representation of a attribute (that returns a string), we need to add an underscore \\_ to its name (e.g. \"lower\\_\" and \"shape\\_\")\n",
    "\n",
    "\n",
    "See https://spacy.io/api/token for the full list of token attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index:    [0, 1, 2, 3, 4, 5]\n",
      "Text:     ['It', 'is', 'so', '90', \"'s\", '!']\n",
      "Lemma:     ['', '', '', '', '', '']\n",
      "Lemma2:     [0, 0, 0, 0, 0, 0]\n",
      "is_alpha: [True, True, True, False, False, False]\n",
      "like_num: [False, False, False, True, False, False]\n",
      "Lowercase:  ['it', 'is', 'so', '90', \"'s\", '!']\n",
      "stopword:  [True, True, True, False, True, False]\n"
     ]
    }
   ],
   "source": [
    "#generally speaking, if we want to generate string, the underscore is needed in the spacy attributes\n",
    "#otherwise without the underscore you get a numerical value, example Lemma2.\n",
    "#TD lemma not working, check spacy VERSION\n",
    "\n",
    "doc = nlp(\"It is so 90's!\")\n",
    "print('Index:   ', [token.i for token in doc])\n",
    "print('Text:    ', [token.text for token in doc])\n",
    "print('Lemma:    ', [token.lemma_ for token in doc])\n",
    "print('Lemma2:    ', [token.lemma for token in doc])\n",
    "print('is_alpha:', [token.is_alpha for token in doc])\n",
    "print('like_num:', [token.like_num for token in doc])\n",
    "print('Lowercase: ', [token.lower_ for token in doc] )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DIY: Token attributes ###\n",
    "Display the different token attributes shown in the following image, for the sentence \"Apple is looking at buying U.K. startup for $1 billion.\". Display the results in the same formatting: for each token in the sentence, print the text of \"token\", \"shape\", \"alpha\" and \"stop\" in the same line.\n",
    "\n",
    "<table> <tr>\n",
    "    <td> <img src=\"img/token_attributes1.png\" alt=\"Drawing\" style=\"width: 100px;\"/> </td>\n",
    "    <td> <img src=\"img/token_attributes2.png\" alt=\"Drawing\" style=\"width: 300px;\"/> </td>\n",
    " </tr> </table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT\tSHAPE\tALPHA\tSTOP\n",
      "Apple\tXxxxx\tTrue\tFalse\n",
      "is\txx\tTrue\tTrue\n",
      "looking\txxxx\tTrue\tFalse\n",
      "at\txx\tTrue\tTrue\n",
      "buying\txxxx\tTrue\tFalse\n",
      "U.K.\tX.X.\tFalse\tFalse\n",
      "startup\txxxx\tTrue\tFalse\n",
      "for\txxx\tTrue\tTrue\n",
      "$\t$\tFalse\tFalse\n",
      "1\td\tFalse\tFalse\n",
      "billion\txxxx\tTrue\tFalse\n",
      ".\t.\tFalse\tFalse\n"
     ]
    }
   ],
   "source": [
    "#TD ex\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion.\")\n",
    "\n",
    "print(\"TEXT\\tSHAPE\\tALPHA\\tSTOP\")\n",
    "#or \n",
    "#print(\"TEXT\", \"SHAPE\", \"ALPHA\", \"STOP\")\n",
    "for token in doc:\n",
    "    print(\"%s\\t%s\\t%s\\t%s\"%(token.text,token.shape_,token.is_alpha,token.is_stop))\n",
    "    #or :  print(token.text, token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT SHAPE ALPHA STOP\n",
      "Apple Xxxxx True False\n",
      "is xx True True\n",
      "looking xxxx True False\n",
      "at xx True True\n",
      "buying xxxx True False\n",
      "U.K. X.X. False False\n",
      "startup xxxx True False\n",
      "for xxx True True\n",
      "$ $ False False\n",
      "1 d False False\n",
      "billion xxxx True False\n",
      ". . False False\n"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion.\")\n",
    "\n",
    "print(\"TEXT\", \"SHAPE\", \"ALPHA\", \"STOP\")\n",
    "for token in doc:\n",
    "    print(token.text, token.shape_, token.is_alpha, token.is_stop)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DIY: Token attributes ###\n",
    "\n",
    "In this example, you’ll use spaCy’s Doc and Token objects, and lexical attributes to find percentages in a text. You’ll be looking for __two subsequent tokens__: a number and a percent sign.\n",
    "\n",
    "Use the __like_num__ token attribute to check whether a token in the doc resembles a number.\n",
    "Get the token following the current token in the document. The index of the next token in the doc is __token.i+1__.\n",
    "Check whether the next token’s text attribute is a percent sign ”%“."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage found: 59\n",
      "Percentage found: 40\n",
      "Percentage found: 1\n"
     ]
    }
   ],
   "source": [
    "#TD inserted\n",
    "\n",
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(\n",
    "    \"Belgium's three official languages are Dutch, spoken by 59% of the population, French, spoken by 40%, \"\n",
    "    \"and German, spoken by less than 1%.\" \n",
    ")\n",
    "\n",
    "\n",
    "# Iterate over the tokens in the doc\n",
    "for token in doc:\n",
    "    # Check if the token resembles a number\n",
    "    if token.like_num == True:\n",
    "        # Get the next token in the document\n",
    "        next_token = doc[token.i+1]\n",
    "        #print(next_token)\n",
    "        # Check if the next token's text equals '%'\n",
    "        if next_token.text == \"%\":\n",
    "            # Print the token\n",
    "            print(\"Percentage found:\", token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage found: 59\n",
      "Percentage found: 40\n",
      "Percentage found: 1\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(\n",
    "    \"Belgium's 3 official languages are Dutch, spoken by 59% of the population, French, spoken by 40%, \"\n",
    "    \"and German, spoken by less than 1%.\" \n",
    ")\n",
    "\n",
    "\n",
    "# Iterate over the tokens in the doc\n",
    "for token in doc:\n",
    "    # Check if the token resembles a number\n",
    "    if token.like_num:\n",
    "        # Get the next token in the document\n",
    "        next_token = doc[token.i+1]\n",
    "        # Check if the next token's text equals '%'\n",
    "        if next_token.text == \"%\":\n",
    "            # Print the token\n",
    "            print(\"Percentage found:\", token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tokenization: Spans ##\n",
    "\n",
    "A Span object is a slice of the document consisting of one or more tokens. It's only a view of the Doc and doesn't contain any data itself.\n",
    "\n",
    "To create a Span, you can use Python's slice notation. For example, 1:4 will create a slice starting from the token at position 1, up to – but not including! – the token at position 4.\n",
    "\n",
    "\n",
    "<img src=\"img/doc_span.png\" alt=\"Drawing\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "doc = nlp(\"Hello world!\")\n",
    "\n",
    "# A slice from the Doc is a Span object\n",
    "span = doc[1:10]\n",
    "#In fact span = doc[0:2] yields the complete string\n",
    "# happens based on tokens, not on chars\n",
    "#no segmentation errors if index number is too large, spacy automatically adapts\n",
    "\n",
    "# Get the span text via the .text attribute\n",
    "print(span)\n",
    "\n",
    "# Try to slice 'world'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DIY: Spans ###\n",
    "Fill in the question marks in the following code based on the instructions given in comments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a movie\n",
      "a movie from South Korea\n"
     ]
    }
   ],
   "source": [
    "# Import the English language class and create the nlp object\n",
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "# Process the text\n",
    "doc = nlp('\"The winner is a movie from South Korea, what the hell was that all about?\" Trump asked.' )\n",
    "\n",
    "# Take a slice of the Doc for \"a movie\" and print it\n",
    "a_movie = doc[4:6]\n",
    "print(a_movie.text)\n",
    "#print(doc.text)\n",
    "#print('Text:    ', [token.text for token in doc])\n",
    "\n",
    "# Print a slice of the Doc for \"a movie from South Korea\" (without the \",\") and print it\n",
    "a_movie_from_sk = doc[4:9]\n",
    "print(a_movie_from_sk.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a movie\n",
      "a movie from South Korea\n"
     ]
    }
   ],
   "source": [
    "# Import the English language class and create the nlp object\n",
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "\n",
    "# Process the text\n",
    "doc = nlp('\"The winner is a movie from South Korea, what the hell was that all about?\" Trump asked.' )\n",
    "\n",
    "# Take a slice of the Doc for \"a movie\" and print it\n",
    "a_movie = doc[4:6]\n",
    "print(a_movie.text)\n",
    "\n",
    "# Print a slice of the Doc for \"a movie from South Korea\" (without the \",\") and print it\n",
    "a_movie_from_sk = doc[4:9]\n",
    "print(a_movie_from_sk.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Statistical Models ##\n",
    "\n",
    "Statistical models enable spaCy to make predictions __in context__. This usually includes part-of speech tags, syntactic dependencies and named entities.\n",
    "\n",
    "These ready-to use models are trained on large datasets of labeled example texts.\n",
    "\n",
    "You can download models from the command line, with different sizes (and accuracies) (https://spacy.io/models/en):<br>\n",
    "\n",
    "Small English model (11 MB): python -m spacy download en_core_web_sm<br>\n",
    "Medium English model (91 MB): python -m spacy download en_core_web_md <br>\n",
    "Large English model (789 MB): python -m spacy download en_core_web_lg<br>\n",
    "\n",
    "You can find the available models for different languages at https://spacy.io/models\n",
    "You can see the Dutch models at https://spacy.io/models/nl\n",
    "\n",
    "The English language class in spacy.lang.en contains the language-specific code and rules included in the library – for example, special case rules for tokenization, stop words or functions to decide whether a word like \"twenty two\" resembles a number.\n",
    "\n",
    "spacy.load(\"en_core_web_sm\") loads the installed statistical model with the shortcut name en – in this case, the en_core_web_sm package. Loading a model will initialize the respective language class (in this case, English), set up the processing pipeline and load in the binary weights of the trained model that allow spaCy to make predictions (e.g. whether a word is a noun or what named entities are in the text). So the nlp object you get back after loading a model is an instance of English, but it also has a processing pipeline set up and weights loaded in.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Statistical Models: POS-tagging ##\n",
    "\n",
    "The process of classifying words into their parts of speech and labeling them accordingly is known as part-of-speech tagging, POS-tagging, or simply tagging. Parts of speech are also known as word classes or lexical categories. The collection of tags used for a particular task is known as a tagset.\n",
    "\n",
    "spaCy maps all language-specific part-of-speech tags to a small, fixed set of word type tags following the Universal Dependencies scheme. The universal tags don’t code for any morphological features and only cover the word type.\n",
    "The following tags are used for English (https://spacy.io/models/en):\n",
    "\n",
    "<table> <tr>\n",
    "    <td> <img src=\"img/postags1.png\" alt=\"Drawing\" style=\"width: 400px;\"/> </td>\n",
    "    <td> <img src=\"img/postags2.png\" alt=\"Drawing\" style=\"width: 400px;\"/> </td>\n",
    " </tr> </table>\n",
    " \n",
    " \n",
    "Most of the tags and labels look pretty abstract, and they vary between languages. __spacy.explain__ will show you a short description – for example, __spacy.explain(\"VBZ\")__ returns “verb, 3rd person singular present”.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" \" PUNCT punctuation `` opening quotation mark\n",
      "The the DET determiner DT determiner\n",
      "winner winner NOUN noun NN noun, singular or mass\n",
      "is be AUX auxiliary VBZ verb, 3rd person singular present\n",
      "a a DET determiner DT determiner\n",
      "movie movie NOUN noun NN noun, singular or mass\n",
      "from from ADP adposition IN conjunction, subordinating or preposition\n",
      "South South PROPN proper noun NNP noun, proper singular\n",
      "Korea Korea PROPN proper noun NNP noun, proper singular\n",
      ", , PUNCT punctuation , punctuation mark, comma\n",
      "what what PRON pronoun WP wh-pronoun, personal\n",
      "the the DET determiner DT determiner\n",
      "hell hell NOUN noun NN noun, singular or mass\n",
      "was be AUX auxiliary VBD verb, past tense\n",
      "that that PRON pronoun DT determiner\n",
      "all all ADV adverb RB adverb\n",
      "about about ADP adposition IN conjunction, subordinating or preposition\n",
      "? ? PUNCT punctuation . punctuation mark, sentence closer\n",
      "\" \" PUNCT punctuation '' closing quotation mark\n",
      "Trump Trump PROPN proper noun NNP noun, proper singular\n",
      "asked ask VERB verb VBD verb, past tense\n",
      ". . PUNCT punctuation . punctuation mark, sentence closer\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the small English model (we don't use the \"English()\" class anymore!)\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "#print(nlp.pipeline) # list of tuples: name, pipeline component\n",
    "\n",
    "\n",
    "# Process a text\n",
    "doc = nlp('\"The winner is a movie from South Korea, what the hell was that all about?\" Trump asked.')\n",
    "\n",
    "# Iterate over the tokens\n",
    "for token in doc:\n",
    "    # Print the text, its lemma, and the predicted part-of-speech tag\n",
    "    # We access these contextual attributes with an underscore (pos_)\n",
    "    print(token.text, token.lemma_, token.pos_, spacy.explain(token.pos_), token.tag_, spacy.explain(token.tag_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DIY: POS-tagging ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "They\tthey\tPRON\tpronoun\tPRP\tpronoun, personal\n",
      "refuse\trefuse\tVERB\tverb\tVBP\tverb, non-3rd person singular present\n",
      "to\tto\tPART\tparticle\tTO\tinfinitival \"to\"\n",
      "permit\tpermit\tVERB\tverb\tVB\tverb, base form\n",
      "us\twe\tPRON\tpronoun\tPRP\tpronoun, personal\n",
      "to\tto\tPART\tparticle\tTO\tinfinitival \"to\"\n",
      "obtain\tobtain\tVERB\tverb\tVB\tverb, base form\n",
      "the\tthe\tDET\tdeterminer\tDT\tdeterminer\n",
      "refuse\trefuse\tNOUN\tnoun\tNN\tnoun, singular or mass\n",
      "permit\tpermit\tNOUN\tnoun\tNN\tnoun, singular or mass\n",
      ".\t.\tPUNCT\tpunctuation\t.\tpunctuation mark, sentence closer\n"
     ]
    }
   ],
   "source": [
    "#TD inserted\n",
    "\n",
    "import spacy\n",
    "\n",
    "# Load the small English model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "text = \"They refuse to permit us to obtain the refuse permit.\" \n",
    "# What should be the part of speech for \"permit\" in both cases?\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "for token in doc:\n",
    "    # Get the lowercase token text, lemma and part-of-speech tags (simple and detailed)\n",
    "    text = token.text\n",
    "    lemma = token.lemma_\n",
    "    pos_simple = token.pos_\n",
    "    pos_detailed = token.tag_\n",
    "    # Get the description of the pos tags using .explain\n",
    "    pos_simple_desc = spacy.explain(token.pos_)\n",
    "    pos_detailed_desc = spacy.explain(token.tag_)\n",
    "    # Print: text, lemma, pos-simple, + \"\\t\" +pos-simple de + \"\\t\" +cription, pos-de + \"\\t\" +ailed, pos-detailed description\n",
    "    # in tab-delimited format\n",
    "    print(text + \"\\t\" + lemma + \"\\t\" + pos_simple + \"\\t\" +  pos_simple_desc + \"\\t\" + pos_detailed + \"\\t\" + pos_detailed_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "They\t-PRON-\tPRON\tpronoun\tPRP\tpronoun, personal\n",
      "refuse\trefuse\tVERB\tverb\tVBP\tverb, non-3rd person singular present\n",
      "to\tto\tPART\tparticle\tTO\tinfinitival to\n",
      "permit\tpermit\tVERB\tverb\tVB\tverb, base form\n",
      "us\t-PRON-\tPRON\tpronoun\tPRP\tpronoun, personal\n",
      "to\tto\tPART\tparticle\tTO\tinfinitival to\n",
      "obtain\tobtain\tVERB\tverb\tVB\tverb, base form\n",
      "the\tthe\tDET\tdeterminer\tDT\tdeterminer\n",
      "refuse\trefuse\tNOUN\tnoun\tNN\tnoun, singular or mass\n",
      "permit\tpermit\tNOUN\tnoun\tNN\tnoun, singular or mass\n",
      ".\t.\tPUNCT\tpunctuation\t.\tpunctuation mark, sentence closer\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the small English model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "text = \"They refuse to permit us to obtain the refuse permit.\" \n",
    "# What should be the part of speech for \"permit\" in both cases?\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "for token in doc:\n",
    "    # Get the lowercase token text, lemma and part-of-speech tags (simple and detailed)\n",
    "    text = token.text\n",
    "    lemma = token.lemma_\n",
    "    pos_simple = token.pos_\n",
    "    pos_detailed = token.tag_\n",
    "    # Get the description of the pos tags using .explain\n",
    "    pos_simple_desc = spacy.explain(token.pos_)\n",
    "    pos_detailed_desc = spacy.explain(token.tag_)\n",
    "    # Print: text, lemma, pos-simple, pos-simple description, pos-detailed, pos-detailed description\n",
    "    # in tab-delimited format\n",
    "    print(text + \"\\t\" + lemma + \"\\t\" + pos_simple + \"\\t\" + pos_simple_desc + \"\\t\" + pos_detailed + \"\\t\" + pos_detailed_desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About spaCy's custom pronoun lemma\n",
    "\n",
    "Unlike verbs and common nouns, there’s no clear base form of a personal pronoun. Should the lemma of “me” be “I”, or should we normalize person as well, giving “it” — or maybe “he”? spaCy’s solution is to introduce a novel symbol, -PRON-, which is used as the lemma for all personal pronouns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Statistical Models: Dependency parsing ##\n",
    "\n",
    "Dependency parsing is the task of extracting a dependency parse of a sentence that represents its grammatical structure and defines the relationships between “head” words and words, which modify those heads.\n",
    "\n",
    "We use Part-of-Speech tagging to label tokens in a sentence with their grammatical word categories as part-of-speech tags. Yet, they do not have any grammatical relations between them. In order to generate the grammatical relations between the tokens, we use linguistic parsers and syntactic dependency parsing is one of them. Via dependency parsing, we create a tree or a graph data structure of a sentence conveying its tokens' grammatical relations. \n",
    "\n",
    "<td> <img src=\"img/dep.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "Dependency relationships are also represented with a universal tagset (For small English model: https://spacy.io/models/en#en_core_web_sm):\n",
    "\n",
    "\n",
    "<table> <tr>\n",
    "    <td> <img src=\"img/deptags1.png\" alt=\"Drawing\" style=\"width: 400px;\"/> </td>\n",
    "    <td> <img src=\"img/deptags2.png\" alt=\"Drawing\" style=\"width: 400px;\"/> </td>\n",
    "    <td> <img src=\"img/deptags3.png\" alt=\"Drawing\" style=\"width: 400px;\"/> </td>\n",
    " </tr> </table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "They\tPRON\tnsubj\n",
      "refuse\tVERB\tROOT\n",
      "to\tPART\taux\n",
      "permit\tVERB\txcomp\n",
      "us\tPRON\tdobj\n",
      "to\tPART\taux\n",
      "obtain\tVERB\txcomp\n",
      "the\tDET\tdet\n",
      "refuse\tNOUN\tcompound\n",
      "permit\tNOUN\tdobj\n",
      ".\tPUNCT\tpunct\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the small English model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "text = \"They refuse to permit us to obtain the refuse permit.\" \n",
    "# What should be the part of speech for \"permit\" in both cases?\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "for token in doc:\n",
    "    # Get the token text, part-of-speech tag and dependency label\n",
    "    #underscore = for string output\n",
    "    token_text = token.text\n",
    "    token_pos = token.pos_\n",
    "    token_dep = token.dep_\n",
    "    # And print all three types of information in tab-delimited format\n",
    "    print(token_text + \"\\t\" + token_pos + \"\\t\" + token_dep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While obtaining word-level dependency labels are easy, they provide linmited information about grammatical relationships. spaCy uses the terms __head__ and __child__ to describe the words connected by a single arc in the dependency tree. \n",
    "\n",
    "Head is the grammatical parent of a given token.\n",
    "Child is the grammatical child of a given token.\n",
    "\n",
    "We can obtain additional information from the dependency trees by using:\n",
    "    - token.head (Why singular?)\n",
    "    - token.children (Why plural?)\n",
    "    \n",
    "\n",
    "<td> <img src=\"img/dep2.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "Now let's print the head of each token, as well as the head's pos-tag and dependency label:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This\tPRON\tnsubj\tis\tAUX\n",
      "is\tAUX\tROOT\tis\tAUX\n",
      "a\tDET\tdet\tsentence\tNOUN\n",
      "sentence\tNOUN\tattr\tis\tAUX\n",
      ".\tPUNCT\tpunct\tis\tAUX\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "text = \"This is a sentence.\"\n",
    "\n",
    "doc=nlp(text)\n",
    "\n",
    "\n",
    "for token in doc:\n",
    "    token_text = token.text\n",
    "    token_pos = token.pos_\n",
    "    token_dep = token.dep_\n",
    "    token_head = token.head.text\n",
    "    token_head_pos = token.head.pos_\n",
    "    token_head_dep = token.head.dep_\n",
    "    print(token_text + \"\\t\" + token_pos + \"\\t\" + token_dep + \"\\t\" + token_head + \"\\t\" + token_head_pos)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This PRON nsubj is AUX ROOT\n",
      "is AUX ROOT is AUX ROOT\n",
      "a DET det sentence NOUN attr\n",
      "sentence NOUN attr is AUX ROOT\n",
      ". PUNCT punct is AUX ROOT\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Import the small English model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "text = \"This is a sentence.\" \n",
    "# What should be the part of speech for \"permit\" in both cases?\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "for token in doc:\n",
    "    # Get the token text, part-of-speech tag and dependency label\n",
    "    token_text = token.text\n",
    "    token_pos = token.pos_\n",
    "    token_dep = token.dep_\n",
    "    token_head = token.head.text # the text of the head token // we refer to ANOTHER token, so underscore not using, but again text attribute\n",
    "    token_head_pos = token.head.pos_ # the pos-tag of the head token\n",
    "    token_head_dep = token.head.dep_ # the dep of the head token\n",
    "    # This is for formatting only\n",
    "    print(token_text, token_pos, token_dep, token_head, token_head_pos, token_head_dep)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "source": [
    "As each token has a single head, it is \"easy\" to refer to them.\n",
    "However, a token can have multiple children. It means that we have to loop over all items in \".children\" object to access different children.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This PRON nsubj\n",
      ">children:\n",
      "<generator object at 0x7fcb721c1520>\n",
      "\n",
      "is AUX ROOT\n",
      ">children:\n",
      "<generator object at 0x7fcb721c1680>\n",
      "This\n",
      "sentence\n",
      ".\n",
      "\n",
      "a DET det\n",
      ">children:\n",
      "<generator object at 0x7fcb721c1520>\n",
      "\n",
      "sentence NOUN attr\n",
      ">children:\n",
      "<generator object at 0x7fcb721c1680>\n",
      "a\n",
      "\n",
      ". PUNCT punct\n",
      ">children:\n",
      "<generator object at 0x7fcb721c1520>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Import the small English model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "text = \"This is a sentence.\" \n",
    "# What should be the part of speech for \"permit\" in both cases?\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "for token in doc:\n",
    "    # Get the token text, part-of-speech tag and dependency label\n",
    "    token_text = token.text\n",
    "    token_pos = token.pos_\n",
    "    token_dep = token.dep_\n",
    "    # print them\n",
    "    print(token_text, token_pos, token_dep)\n",
    "    print(\">children:\")\n",
    "    # Get the syntactic children of the token\n",
    "    token_children = token.children\n",
    "    print(token_children)\n",
    "    for c in token_children:\n",
    "        # Print the text of each child\n",
    "        #print(c)\n",
    "        print(c.text)\n",
    "    print()\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok we get much more information about the grammatical relationships but it is still difficult to have a visual understanding.\n",
    "Spacy also offers a visualization tool: displacy! \n",
    "\n",
    "See a full list of all the options you can use with displacy:\n",
    "https://spacy.io/api/top-level#displacy_options\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "#'display'...\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"This is a second sentence.\")\n",
    "displacy.render(doc, style=\"dep\")\n",
    "\n",
    "#displacy.render(doc, style=\"dep\", options={'compact':True, 'font':'Arial'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Statistical Models: Named-entity recognition ##\n",
    "\n",
    "Named entity recognition (NER) seeks to locate and classify named entities in text into pre-defined categories such as the names of persons, organizations, locations, expressions of times, quantities, monetary values, percentages, etc. NER is used in many fields in Natural Language Processing (NLP), and it can help answering many real-world questions, such as:\n",
    "- Which companies were mentioned in the news article?\n",
    "- Were specified products mentioned in complaints or reviews?\n",
    "- Does the tweet contain the name of a person? Does the tweet contain this person’s location?\n",
    "\n",
    "Spacy's NER models trained on the [OntoNotes 5](https://catalog.ldc.upenn.edu/LDC2013T19) corpus support the following entity types:\n",
    "\n",
    "<td> <img src=\"img/ner.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "Because models are statistical and strongly depend on the examples they were trained on, this doesn’t always work perfectly and might need some tuning later, depending on your use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n",
      "first ORDINAL\n",
      "U.S. GPE\n",
      "$1 trillion MONEY\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = \"It’s official: Apple is the first U.S. public company to reach a $1 trillion market value\"\n",
    "doc = nlp(text)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n",
      "first ORDINAL\n",
      "U.S. GPE\n",
      "$1 trillion MONEY\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"It’s official: Apple is the first U.S. public company to reach a $1 trillion market value\"\n",
    "\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Iterate over the predicted entities\n",
    "for ent in doc.ents:\n",
    "    # Print the entity text and its label\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DIY: Named Entity Recognition ###\n",
    "\n",
    "Write a code to:\n",
    "1. Open \"austen-emma.txt\" \n",
    "2. Store the content of the document in a single line (Why?) - Also try your code without this step.\n",
    "3. For the first 5 sentences in this document, using Spacy, print: (1) the sentence number, (2) the sentence itself, (3) the words that are detected as names in the sentence and (4) their corresponding NER labels.\n",
    "4. Try to match the following output. Pay attention to the formatting.\n",
    "\n",
    "\n",
    "#### Output ####\n",
    "\\*\\*\\* SENT 1 \\*\\*\\*<br>\n",
    "[Emma by Jane Austen 1816]  VOLUME I<br>\n",
    "\\*\\*\\* ENTITIES 1 \\*\\*\\*<br>\n",
    "Jane Austen PERSON<br>\n",
    "-----<br>\n",
    "\\*\\*\\* SENT 2 \\*\\*\\*<br>\n",
    "CHAPTER I   Emma Woodhouse, handsome, clever, and rich, with a comfortable home and happy disposition, seemed to unite some of the best blessings of existence; and had lived nearly twenty-one years in the world with very little to distress or vex her.<br>\n",
    "\\*\\*\\* ENTITIES 2 \\*\\*\\*<br>\n",
    "Emma Woodhouse PERSON<br>\n",
    "nearly twenty-one years DATE<br>\n",
    "-----<br>\n",
    "\\*\\*\\* SENT 3 \\*\\*\\*<br>\n",
    "She was the youngest of the two daughters of a most affectionate, indulgent father; and had, in consequence of her sister's marriage, been mistress of his house from a very early period.<br>\n",
    "\\*\\*\\* ENTITIES 3\\*\\*\\*<br>\n",
    "two CARDINAL<br>\n",
    "-----<br>\n",
    "\\*\\*\\* SENT 4 \\*\\*\\*<br>\n",
    "Her mother had died too long ago for her to have more than an indistinct remembrance of her caresses; and her place had been supplied by an excellent woman as governess, who had fallen little short of a mother in affection.<br>\n",
    "\\*\\*\\* ENTITIES 4 \\*\\*\\*<br>\n",
    "-----<br>\n",
    "\\*\\*\\* SENT 5 \\*\\*\\*<br>\n",
    "Sixteen years had Miss Taylor been in Mr. Woodhouse's family, less as a governess than a friend, very fond of both daughters, but particularly of Emma.<br>\n",
    "\\*\\*\\* ENTITIES 5 \\*\\*\\*<br>\n",
    "Sixteen years DATE<br>\n",
    "Taylor PERSON<br>\n",
    "Woodhouse PERSON<br>\n",
    "Emma PERSON<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TD inserted\n",
    "import spacy\n",
    "import codecs\n",
    "import sys\n",
    "\n",
    "#file = open('./austen-emma.txt','r')\n",
    "\n",
    "#for line in file:\n",
    "#    print(line)\n",
    "\n",
    "\n",
    "\n",
    "textfile = codecs.open('./austen-emma.txt','r','utf-8').read()\n",
    "#storing everything in one line!\n",
    "textfile_line = textfile.replace('\\n', ' ')\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "doc=nlp(textfile_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent 1\n",
      "[Emma by Jane Austen 1816]\n",
      "\n",
      "Emma Woodhouse, handsome, clever, and rich, with a comfortable home\n",
      "and happy disposition, seemed to unite some of the best blessings\n",
      "of existence; and had lived nearly twenty-one years in the world\n",
      "with very little to distress or vex her.\n",
      "Emma PERSON\n",
      "Jane Austen 1816 PERSON\n",
      "Emma Woodhouse PERSON\n",
      "nearly twenty-one years CARDINAL\n",
      "sent 2\n",
      "She was the youngest of the two daughters of a most affectionate,\n",
      "indulgent father; and had, in consequence of her sister's marriage,\n",
      "been mistress of his house from a very early period.\n",
      "two CARDINAL\n",
      "sent 3\n",
      "Her mother\n",
      "had died too long ago for her to have more than an indistinct\n",
      "remembrance of her caresses; and her place had been supplied\n",
      "by an excellent woman as governess, who had fallen little short\n",
      "of a mother in affection.\n",
      "indistinct ORG\n",
      "sent 4\n",
      "Sixteen years had Miss Taylor been in Mr. Woodhouse's family,\n",
      "less as a governess than a friend, very fond of both daughters,\n",
      "but particularly of Emma.\n",
      "Sixteen years DATE\n",
      "Taylor PERSON\n",
      "Woodhouse PERSON\n",
      "Emma ORG\n",
      "sent 5\n",
      "Between _them_\n"
     ]
    }
   ],
   "source": [
    "#TD inserted\n",
    "\n",
    "counter = 0\n",
    "\n",
    "\n",
    "#for sent in doc.sents:\n",
    "for sent in list(doc.sents)[:5]:    \n",
    "    counter+=1\n",
    "    print(\"sent \" + str(counter))\n",
    "    print(sent.text.strip())\n",
    "    for ent in sent.ents:\n",
    "        #print(ent.text,ent.label_)\n",
    "        print(ent.text,ent.label_)\n",
    "        #print(ent.text,ent.label_,spacy.explain(ent.label_))\n",
    "        #print(ent.text, ent.label_, spacy.explain(ent.label_))\n",
    "    #if counter == 5:\n",
    "    #    sys.exit()\n",
    "\n",
    "#textfile = codecs.open('./austen-emma.txt','r','utf-8').read()\n",
    "#for line in textfile:\n",
    "#    print(line)\n",
    "\n",
    "\n",
    "#alternative for sys.exit:\n",
    "#use index, however, doc is a generator and should first be casted to list,\n",
    "# so\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import spacy\n",
    "import sys\n",
    "\n",
    "#WITHOUT REPLACING NEWLINES BY SPACE\n",
    "textfile = codecs.open(\"./austen-emma.txt\", \"r\", \"utf-8\").read()\n",
    "#textfile_line = textfile.replace('\\n', ' ')\n",
    "#if above line is not inserted, system will activate sentence segmenter\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(textfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** SENTENCE 1 ***\n",
      "[Emma by Jane Austen 1816]\n",
      "\n",
      "Emma Woodhouse, handsome, clever, and rich, with a comfortable home\n",
      "and happy disposition, seemed to unite some of the best blessings\n",
      "of existence; and had lived nearly twenty-one years in the world\n",
      "with very little to distress or vex her.\n",
      "*** ENTITIES 1 ***\n",
      "Emma PERSON People, including fictional\n",
      "Jane Austen 1816 PERSON People, including fictional\n",
      "Emma Woodhouse PERSON People, including fictional\n",
      "nearly twenty-one years CARDINAL Numerals that do not fall under another type\n",
      "-----\n",
      "*** SENTENCE 2 ***\n",
      "She was the youngest of the two daughters of a most affectionate,\n",
      "indulgent father; and had, in consequence of her sister's marriage,\n",
      "been mistress of his house from a very early period.\n",
      "*** ENTITIES 2 ***\n",
      "two CARDINAL Numerals that do not fall under another type\n",
      "-----\n",
      "*** SENTENCE 3 ***\n",
      "Her mother\n",
      "had died too long ago for her to have more than an indistinct\n",
      "remembrance of her caresses; and her place had been supplied\n",
      "by an excellent woman as governess, who had fallen little short\n",
      "of a mother in affection.\n",
      "*** ENTITIES 3 ***\n",
      "indistinct ORG Companies, agencies, institutions, etc.\n",
      "-----\n",
      "*** SENTENCE 4 ***\n",
      "Sixteen years had Miss Taylor been in Mr. Woodhouse's family,\n",
      "less as a governess than a friend, very fond of both daughters,\n",
      "but particularly of Emma.\n",
      "*** ENTITIES 4 ***\n",
      "Sixteen years DATE Absolute or relative dates or periods\n",
      "Taylor PERSON People, including fictional\n",
      "Woodhouse PERSON People, including fictional\n",
      "Emma ORG Companies, agencies, institutions, etc.\n",
      "-----\n",
      "*** SENTENCE 5 ***\n",
      "Between _them_\n",
      "*** ENTITIES 5 ***\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for sent in list(doc.sents)[:5]:\n",
    "    counter+=1\n",
    "    print(\"*** SENTENCE \"+str(counter)+\" ***\")\n",
    "    print(sent.text.strip())\n",
    "    print(\"*** ENTITIES \"+str(counter)+\" ***\")\n",
    "    for ent in sent.ents:\n",
    "        print(ent.text, ent.label_, spacy.explain(ent.label_))\n",
    "        \n",
    "    print(\"-----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import spacy\n",
    "import sys\n",
    "\n",
    "\n",
    "textfile = codecs.open(\"./austen-emma.txt\", \"r\", \"utf-8\").read()\n",
    "textfile_line = textfile.replace('\\n', ' ')\n",
    "#if above line is not inserted, system will activate sentence segmenter\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(textfile_line)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** SENTENCE 1 ***\n",
      "[Emma by Jane Austen 1816]\n",
      "\n",
      "Emma Woodhouse, handsome, clever, and rich, with a comfortable home\n",
      "and happy disposition, seemed to unite some of the best blessings\n",
      "of existence; and had lived nearly twenty-one years in the world\n",
      "with very little to distress or vex her.\n",
      "*** ENTITIES 1 ***\n",
      "Emma PERSON People, including fictional\n",
      "Jane Austen 1816 PERSON People, including fictional\n",
      "Emma Woodhouse PERSON People, including fictional\n",
      "nearly twenty-one years CARDINAL Numerals that do not fall under another type\n",
      "-----\n",
      "*** SENTENCE 2 ***\n",
      "She was the youngest of the two daughters of a most affectionate,\n",
      "indulgent father; and had, in consequence of her sister's marriage,\n",
      "been mistress of his house from a very early period.\n",
      "*** ENTITIES 2 ***\n",
      "two CARDINAL Numerals that do not fall under another type\n",
      "-----\n",
      "*** SENTENCE 3 ***\n",
      "Her mother\n",
      "had died too long ago for her to have more than an indistinct\n",
      "remembrance of her caresses; and her place had been supplied\n",
      "by an excellent woman as governess, who had fallen little short\n",
      "of a mother in affection.\n",
      "*** ENTITIES 3 ***\n",
      "indistinct ORG Companies, agencies, institutions, etc.\n",
      "-----\n",
      "*** SENTENCE 4 ***\n",
      "Sixteen years had Miss Taylor been in Mr. Woodhouse's family,\n",
      "less as a governess than a friend, very fond of both daughters,\n",
      "but particularly of Emma.\n",
      "*** ENTITIES 4 ***\n",
      "Sixteen years DATE Absolute or relative dates or periods\n",
      "Taylor PERSON People, including fictional\n",
      "Woodhouse PERSON People, including fictional\n",
      "Emma ORG Companies, agencies, institutions, etc.\n",
      "-----\n",
      "*** SENTENCE 5 ***\n",
      "Between _them_\n",
      "*** ENTITIES 5 ***\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "\n",
    "counter = 0\n",
    "for sent in list(doc.sents)[:5]:\n",
    "    counter+=1\n",
    "    print(\"*** SENTENCE \"+str(counter)+\" ***\")\n",
    "    print(sent.text.strip())\n",
    "    print(\"*** ENTITIES \"+str(counter)+\" ***\")\n",
    "    for ent in sent.ents:\n",
    "        print(ent.text, ent.label_, spacy.explain(ent.label_))\n",
    "        \n",
    "    print(\"-----\")\n",
    "    #if counter == 5:\n",
    "        #sys.exit()\n",
    "\n",
    "#Between _them_ it was more the intimacy of sisters. \n",
    "#TD\n",
    "#spacy.displacy.render(doc, style=\"ent\")\n",
    "#spacy.displacy.render(list(doc.sents)[0], style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">[\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Emma\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " by \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Jane Austen 1816\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       "]  Emma Woodhouse, handsome, clever, and rich, with a comfortable home and happy disposition, seemed to unite some of the best blessings of existence; and had lived \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    nearly twenty-one years\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " in the world with very little to distress or vex her. </div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\"> She was the youngest of the \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    two\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " daughters of a most affectionate, indulgent father; and had, in consequence of her sister's marriage, been mistress of his house from a very early period. </div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\"> Her mother had died too long ago for her to have more than an indistinct remembrance of her caresses; and her place had been supplied by an excellent woman as governess, who had fallen little short of a mother in affection. </div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\"> \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Sixteen years\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " had Miss \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Taylor\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " been in Mr. \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Woodhouse\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       "'s family, less as a governess than a friend, very fond of both daughters, but particularly of \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Emma\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ". </div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\"> Between _them_ </div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#TD inserted\n",
    "for sent in list(doc.sents)[:5]:\n",
    "    spacy.displacy.render(sent, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use displacy also to visualise named-entities!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Local health officials in the \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Iraqi\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Shia\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       " city of \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Najaf\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " said an \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Iranian\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       " theology student was the \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    first\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORDINAL</span>\n",
       "</mark>\n",
       " positive case of the virus, \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Reuters\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " reports.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "doc = nlp(\"Local health officials in the Iraqi Shia city of Najaf said \\\n",
    "an Iranian theology student was the first positive case of the virus, Reuters reports.\")\n",
    "spacy.displacy.render(doc, style='ent')\n",
    "#outside jup notebook\n",
    "#spacy.displacy.serve(doc, style='ent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DIY. Named entity recognition - visualisation ###\n",
    "Modify the code for the previous DIY so that instead of printing out the entities and their labels, visualise them for each sentence using displacy!\n",
    "\n",
    "For each sentence, this code will only:\n",
    "- print the sentence number\n",
    "- visualize entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** SENT 1 ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">[Emma by \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Jane Austen\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    1816\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       "]\n",
       "\n",
       "Emma Woodhouse, handsome, clever, and rich, with a comfortable home\n",
       "and happy disposition, seemed to unite some of the best blessings\n",
       "of existence; and had lived nearly twenty-one years in the world\n",
       "with very little to distress or vex her.\n",
       "\n",
       "</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** SENT 2 ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">She was the youngest of the \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    two\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " daughters of a most affectionate,\n",
       "indulgent father; and had, in consequence of her sister's marriage,\n",
       "been mistress of his house from a very early period.  </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** SENT 3 ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W006] No entities to visualize found in Doc object. If this is surprising to you, make sure the Doc was processed using a model that supports named entity recognition, and check the `doc.ents` property manually if necessary.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Her mother\n",
       "had died too long ago for her to have more than an indistinct\n",
       "remembrance of her caresses; and her place had been supplied\n",
       "by an excellent woman as governess, who had fallen little short\n",
       "of a mother in affection.\n",
       "\n",
       "</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** SENT 4 ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Sixteen years\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " had Miss \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Taylor\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " been in Mr. \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Woodhouse\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       "'s family,</br>less as a governess than a friend, very fond of both daughters,</br>but particularly of \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Emma\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ".  </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** SENT 5 ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W006] No entities to visualize found in Doc object. If this is surprising to you, make sure the Doc was processed using a model that supports named entity recognition, and check the `doc.ents` property manually if necessary.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Between _them</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import codecs\n",
    "import spacy\n",
    "import sys\n",
    "\n",
    "# Write your code here\n",
    "textfile = codecs.open(\"./austen-emma.txt\", \"r\", \"utf-8\").read()\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(textfile[:1000])\n",
    "\n",
    "counter = 0\n",
    "for sent in list(doc.sents)[:5]:\n",
    "    counter+=1\n",
    "    print(\"*** SENT \"+str(counter)+\" ***\")\n",
    "    spacy.displacy.render(sent, style='ent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though we have the tables that summarize the different labels we alreadh have seen (POS-labels, dependency labels), we can also dynamically get their explanations with spacy.\n",
    "See the following example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(u\"Local health officials in the Iraqi Shia city of Najaf said \\\n",
    "an Iranian theology student was the first positive case of the virus, Reuters reports.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, spacy.explain(token.pos_))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DIY: Spacy.explain() ###\n",
    "\n",
    "Modify the code in the previous DIY (Named entity recognition - visualisation) so that after visualizing the named entities in the sentence, it also prints each NER label and the corrsponding explanation for each label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(u\"Local health officials in the Iraqi Shia city of Najaf said \\\n",
    "an Iranian theology student was the first positive case of the virus, Reuters reports.\")\n",
    "\n",
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have been accessing the entities in a Spacy document (doc), as entities can span multiple tokens.<br>\n",
    "We can also access the named entity label each token belongs to.<br>\n",
    "To do this we need to use token attribute (and not a document attribute): __ent\\_type\\___<br>\n",
    "Note: ent\\_type\\_ will return an empty string if there are not entities.<br>\n",
    "\n",
    "We can also see whether the token __starts, continues an entity or is outside an entity (no entity)__, using: __ent\\_iob\\___<br>\n",
    "- B: begin\n",
    "- I: inside\n",
    "- O: outside (not a part of an entity)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local\t\tO\n",
      "health\t\tO\n",
      "officials\t\tO\n",
      "in\t\tO\n",
      "the\t\tO\n",
      "Iraqi\tNORP\tB\n",
      "Shia\tNORP\tB\n",
      "city\t\tO\n",
      "of\t\tO\n",
      "Najaf\tGPE\tB\n",
      "said\t\tO\n",
      "an\t\tO\n",
      "Iranian\tNORP\tB\n",
      "theology\t\tO\n",
      "student\t\tO\n",
      "was\t\tO\n",
      "the\t\tO\n",
      "first\tORDINAL\tB\n",
      "positive\t\tO\n",
      "case\t\tO\n",
      "of\t\tO\n",
      "the\t\tO\n",
      "virus\t\tO\n",
      ",\t\tO\n",
      "Reuters\tORG\tB\n",
      "reports\t\tO\n",
      ".\t\tO\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(\"Local health officials in the Iraqi Shia city of Najaf said \\\n",
    "an Iranian theology student was the first positive case of the virus, Reuters reports.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text + \"\\t\" + token.ent_type_ + \"\\t\" + token.ent_iob_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iraqi\tNORP\tB\n",
      "Shia\tNORP\tB\n",
      "Najaf\tGPE\tB\n",
      "Iranian\tNORP\tB\n",
      "first\tORDINAL\tB\n",
      "Reuters\tORG\tB\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DIY: token entities ###\n",
    "\n",
    "Modify the code in the previous code to loop over all tokens of a doc and print the token text, named entity type and named entity location (iob), only for the tokens which are named entities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iraqi\tNORP\tB\n",
      "Shia\tNORP\tB\n",
      "Najaf\tGPE\tB\n",
      "Iranian\tNORP\tB\n",
      "first\tORDINAL\tB\n",
      "Reuters\tORG\tB\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(u\"Local health officials in the Iraqi Shia city of Najaf said \\\n",
    "an Iranian theology student was the first positive case of the virus, Reuters reports.\")\n",
    "\n",
    "# Add your code here\n",
    "for token in doc:\n",
    "    if token.ent_iob_ == \"O\":\n",
    "        continue\n",
    "    else:\n",
    "        print(token.text + \"\\t\" + token.ent_type_ + \"\\t\" + token.ent_iob_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iraqi\tNORP\tB\n",
      "Shia\tNORP\tB\n",
      "Najaf\tGPE\tB\n",
      "Iranian\tNORP\tB\n",
      "first\tORDINAL\tB\n",
      "Reuters\tORG\tB\n"
     ]
    }
   ],
   "source": [
    "#TD inserted ALTERNATIVE SOLUTION:\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(\"Local health officials in the Iraqi Shia city of Najaf said \\\n",
    "an Iranian theology student was the first positive case of the virus, Reuters reports.\")\n",
    "\n",
    "for token in doc:\n",
    "    if len(token.ent_type_) > 0:\n",
    "      print(token.text + \"\\t\" + token.ent_type_ + \"\\t\" + token.ent_iob_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
